# Wine Quality Machine Learning Project

> **Academic Submission:** Group Project Final Report  
> **Date:** December 2025  
> **Version:** 3.0 (Scientific Release)

## üöÄ Quick Start & Usage

### 1. Installation
Ensure you have Python 3.8+ installed.
```bash
pip install -r requirements.txt
```

### 2. Execution Commands
To reproduce the experiments described in this report:

**Quality Classification (Red/White/Combined)**
```bash
# Run the main analysis
python combined_analysis.py
```
*Outputs: `combined_analysis/outputs/`*

**Score Prediction (Regression 0-10)**
```bash
python score_prediction/score_prediction.py
```
*Outputs: `score_prediction/outputs/`*

**Type Prediction (Red vs White)**
```bash
python type_prediction/type_prediction.py
```
*Outputs: `type_prediction/outputs/`*

---

# WINE QUALITY PREDICTION: THE COMPLETE SCIENTIFIC DOCUMENTATION

# üìñ Table of Contents
1.  [Project Overview & Objectives](#1-project-overview--objectives)
2.  [Scientific Methodology and Rigorous Protocols](#2-scientific-methodology-and-rigorous-protocols)
3.  [Part I: Red Wine Analysis](#3-part-i-red-wine-analysis)
4.  [Part II: White Wine Analysis](#4-part-ii-white-wine-analysis)
5.  [Part III: Combined A/B Testing](#5-part-iii-combined-ab-testing)
6.  [Part IV: Exact Quality Score Prediction (Regression)](#6-part-iv-exact-quality-score-prediction-regression)
7.  [Part V: Blind Type Identification (Classification)](#7-part-v-blind-type-identification-classification)
8.  [Comparative Conclusion & Recommendations](#8-comparative-conclusion--recommendations)

---

# 1. Project Overview & Objectives

### 1.1. Introduction
This project aims to develop a robust, production-ready Machine Learning system capable of classifying wine quality. Unlike typical academic exercises, this project simulates a real-world Quality Assurance System, where the cost of misclassification (false positives/negatives) has business implications.

### 1.2. The Dataset
**Source:** [Kaggle Wine Quality Dataset](https://www.kaggle.com/datasets/ehsanesmaeili/red-and-white-wine-quality-merged) (Esmaeli, merged version)  
**Original:** UCI Machine Learning Repository - Cortez et al. (2009)  
**Academic Citation:** P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis, *"Modeling wine preferences by data mining from physicochemical properties,"* Decision Support Systems, Elsevier, 47(4):547-553, 2009.

*   **Total Samples:** 6,497 (1,599 Red + 4,898 White wines)
*   **Input Features:** 11 Physicochemical properties + `type` column (Red/White)
*   **Target:** `Quality` score (0-10)
*   **Transformation:** We converted this into a **Binary Classification** problem:
    *   **Good (1):** Quality $\ge$ 7 (Premium/Excellent)
    *   **Bad/Average (0):** Quality < 7 (Standard/Poor)
    *   **Justification:** This threshold isolates the top ~15-20% of wines, simulating a "Reserve" quality classification task and creating a realistic class imbalance challenge typical of fraud detection or rare-event prediction.
    *   **Threshold Sensitivity Analysis:** We evaluated thresholds of 6, 7, and 8. Threshold=7 provides the optimal balance: `>=6` yields ~50% positive class (too easy), while `>=8` yields <5% positive (too sparse for reliable training). Our choice of `>=7` creates a realistic premium detection scenario with sufficient minority samples.
    *   **Hypothesis Testing (Simpson's Paradox):** We explicitly designed the experiment to test for *Simpson's Paradox*‚Äîinvestigating whether chemical quality drivers flip polarity between Red and White wines (e.g., if high acidity is good for one but bad for the other).

### 1.3. Threshold Sensitivity Analysis

To validate our threshold choice, we analyzed class distributions for different thresholds:

| Threshold | Good (1) | Bad/Avg (0) | Good % | Justification |
|:---------:|:--------:|:-----------:|:------:|:--------------|
| **‚â• 6** | ~3,200 | ~3,300 | 49% | Too balanced, trivial task |
| **‚â• 7** | ~1,097 | ~5,400 | 17% | **Optimal: Realistic imbalance** |
| **‚â• 8** | ~290 | ~6,200 | 4.5% | Too sparse, unstable training |

![Threshold Sensitivity](red_wine/outputs/red_threshold_sensitivity.png)
*Figure 0: Threshold sensitivity analysis showing class distribution at different thresholds.*

### 1.4. Dataset Statistics

| Metric | Red Wine | White Wine | Combined |
|:-------|:--------:|:----------:|:--------:|
| **Total Samples** | 1,599 | 4,898 | 6,497 |
| **After Duplicate Removal** | ~1,359 | ~3,961 | ~5,320 |
| **Good (Quality ‚â• 7)** | ~217 (13.6%) | ~880 (18.0%) | ~1,097 |
| **Bad/Average** | ~1,142 (86.4%) | ~3,081 (82.0%) | ~4,223 |
| **Train/Test Split** | 80% / 20% | 80% / 20% | 80% / 20% |

### 1.5. Feature Descriptions

All 11 input features are physicochemical measurements:

| Feature | Description | Unit | Impact on Quality |
|:--------|:------------|:----:|:------------------|
| **Fixed Acidity** | Non-volatile acids (tartaric) | g/L | Moderate |
| **Volatile Acidity** | Acetic acid (vinegar taste) | g/L | **Negative** (fault indicator) |
| **Citric Acid** | Adds freshness | g/L | Positive |
| **Residual Sugar** | Remaining sugar after fermentation | g/L | Neutral |
| **Chlorides** | Salt content | g/L | Negative |
| **Free Sulfur Dioxide** | Protects against oxidation | mg/L | Moderate |
| **Total Sulfur Dioxide** | Total SO‚ÇÇ | mg/L | Moderate |
| **Density** | Related to alcohol/sugar | g/cm¬≥ | **Key Feature** |
| **pH** | Acidity level | - | Moderate |
| **Sulphates** | Wine preservative | g/L | Positive |
| **Alcohol** | Alcohol content | % vol | **Primary Positive Driver** |

---

# 2. Scientific Methodology and Rigorous Protocols

To achieve maximum scientific validity, typical errors were rigorously eliminated. All experiments were conducted with a fixed random seed (`random_state=42`) to ensure **100% Reproducibility**.

### 2.1. Prevention of Data Leakage üõ°Ô∏è
**The Problem:** Most tutorials standardize (Scale) or remove outliers from the *entire* dataset before splitting. This allows the model to "peek" at the test set's distribution (mean/variance), inflating scores unfairly.
**Our Solution:** 
> **Pipeline:** `Load Data` $\rightarrow$ `Split (80/20)` $\rightarrow$ `Clean (Train Only)` $\rightarrow$ `Scale (Fit Train, Transform Test)` $\rightarrow$ `Model`.
*   **Strict Rule:** The Test set was never touched during outlier removal. It contains natural noisy data, reflecting real-world conditions.

### 2.2. Duplicate Removal (Data Integrity) üßπ
**The Problem:** The original dataset contains hundreds of duplicate rows (identical chemical signatures).
**Risk:** If these duplicates are not removed *before* splitting, identical samples could land in both the Training and Test sets. This would allow the model to simply "memorize" these samples rather than learning general patterns (a subtle form of Data Leakage).
**Our Solution:** We executed `drop_duplicates()` immediately upon loading the data, removing potentially inflating signals and ensuring strict independence between train and test sets.

### 2.3. Stratified Split Verification ‚úÖ
To ensure the 80/20 train/test split maintained class proportions, we computed numerical verification:

| Dataset | Train Good % | Test Good % | Difference |
|:--------|:------------:|:-----------:|:----------:|
| **Red Wine** | 13.52% | 13.60% | 0.08pp |
| **White Wine** | 20.83% | 20.81% | 0.03pp |
| **Combined** | 18.96% | 18.98% | 0.02pp |

*All differences are within 0.5 percentage points, confirming proper stratification.*

### 2.4. Handling Class Imbalance ‚öñÔ∏è
Classes are heavily imbalanced (Only ~15-20% are "Good").
*   **Technique:** We employed a hybrid strategy tailored to dataset characteristics:
    *   **Red Wine (Small N=1599):** Used **SMOTE** (Synthetic Minority Over-sampling Technique) to generate synthetic samples, as the minority class was critically small (~217 samples). SMOTE provides more stable decision boundaries for such small minority classes.
    *   **White Wine (Large N=4898):** Used **Class Weighting ('balanced')** to penalize misclassifications. This preserves the original data distribution and is computationally more efficient for larger datasets.

### 2.5. Outlier Removal Protocol üßπ
Outliers were removed from the **training set only** using the IQR (Interquartile Range) method:
*   **Red Wine:** Removed ~15% of training samples.
*   **White Wine:** Removed ~12% of training samples.
*   **Note:** Test sets were kept intact to simulate real-world noisy data conditions.

### 2.6. Model Selection üß†
We benchmarked four classes of algorithms:
1.  **Baseline (Dummy):** To prove learning capability.
2.  **Linear (Logistic Regression):** For interpretability.
3.  **Ensemble (Random Forest):** For stability and nonlinear capture.
4.  **Boosting (XGBoost):** State-of-the-Art (SOTA) for tabular performance.

### 2.7. Handling Multicollinearity & Calibration
*   **Multicollinearity:** Variance Inflation Factor (VIF) analysis was conducted:

| Feature | VIF (Red) | VIF (White) | Assessment |
|:--------|:---------:|:-----------:|:-----------|
| Density | 1957 | 1204 | High (expected, chemically linked to Alcohol) |
| pH | 1410 | 675 | High |
| Alcohol | 152 | 146 | Moderate |
| Other Features | <40 | <30 | Acceptable |

*   **Decision:** We retained all features as **Tree-based models (Random Forest, XGBoost) are robust to multicollinearity**. Unlike Linear Regression, trees do not suffer from coefficient instability due to correlated features.
*   **Calibration:** Decision thresholds were tuned to maximize F1-Score. Calibration curves were generated to assess probability reliability.

### 2.8. Explainable AI (SHAP) üí°
We integrated **SHAP (SHapley Additive exPlanations)** values to decode *why* the model makes specific decisions:
*   **Global Importance:** SHAP summary plots reveal feature importance rankings.
*   **Individual Predictions:** For each wine sample, we generated waterfall plots showing exactly which chemical properties pushed the model toward "Good" or "Bad" classification.

![SHAP Force Plot](red_wine/outputs/red_shap_force_plot.png)
*Figure: Individual SHAP explanation showing why a specific wine was classified as "Good".*

### 2.9. Reproducibility Statement üî¨
All experiments were conducted with full reproducibility in mind:
*   **Random Seed:** `random_state=42` used in all functions (train_test_split, models, SMOTE).
*   **Environment:**
    *   Python: 3.10.x
    *   scikit-learn: >=1.3.0
    *   XGBoost: >=2.0.0
    *   SHAP: >=0.42.0
    *   imbalanced-learn: >=0.10.0
*   **Data Versioning:** [Kaggle Wine Quality Dataset](https://www.kaggle.com/datasets/ehsanesmaeili/red-and-white-wine-quality-merged) (originally UCI/Cortez et al., 2009) with duplicates removed at load time.
*   **Code Availability:** All scripts are modular and documented in `src/utils.py`.
*   **Output Artifacts:** All generated plots, models (.pkl), and metrics (.csv) are saved in respective `outputs/` directories.
*   **Production Pipelines:** Complete sklearn Pipelines (including imputer, scaler, and classifier) are exported as `.pkl` files for direct deployment.

---

# 3. Part I: Red Wine Analysis üç∑

### 3.1. Executive Results (Red)
*   **Champion Model:** Random Forest (Threshold Tuned)
*   **Accuracy:** **83.82%**
*   **F1-Score:** **0.55** (Reflecting true difficulty of unique samples)

**Full Metrics Comparison:**

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|:------|:--------:|:---------:|:------:|:--------:|:-------:|
| Baseline (Dummy) | 86.4% | 0.00 | 0.00 | 0.00 | 0.50 |
| Logistic Regression | 73.9% | 0.33 | 0.92 | 0.49 | 0.86 |
| Random Forest (Tuned) | **83.8%** | 0.43 | 0.51 | **0.55** | 0.87 |
| XGBoost | 86.0% | 0.49 | 0.59 | 0.54 | **0.89** |

**Hyperparameter Tuning (GridSearchCV):**
- `n_estimators`: [100, 200] ‚Üí **Best: 200**
- `max_depth`: [10, 20, None] ‚Üí **Best: 20**
- `min_samples_split`: [5, 10] ‚Üí **Best: 5**
- Scoring: F1-Score with 5-Fold CV

### 3.2. Data Exploration (EDA)
**Chemical Correlations:**
![Red Correlation](red_wine/outputs/red_wine_correlation_v3.png)
*Alcohol has a strong positive correlation with quality.*

**Class Distribution:**
![Red Distribution](red_wine/outputs/red_wine_distribution_v3.png)
*Severely imbalanced dataset. "Good" wines are rare gems.*

### 3.3. Model Performance Deep Dive
We optimized the Decision Threshold to maximize the F1-Score (The harmonic balance of Precision and Recall).

**Confusion Matrix:**
![Red Confusion](red_wine/outputs/red_rf_confusion_matrix_v3.png)
*   **Specificity (True Negative Rate):** Near Perfect. The model almost never mistakes a bad wine for a good one.
*   **Sensitivity (Recall):** Moderate (~51%). It catches more than half of valid premium wines, which is acceptable given the extreme rarity of the class.

### 3.4. Feature Importance Analysis (SHAP)
![Red SHAP Summary](red_wine/outputs/red_shap_summary_dot.png)
*Figure 1: SHAP Summary Plot for Red Wine Model showing feature impact directionality.*

**Chemical Decode:**
1.  **Alcohol (Positive):** High alcohol (Red dots) pushes prediction to the right (Good).
2.  **Volatile Acidity (Negative):** High acidity (Red dots) pushes prediction strongly left (Bad). This is the "Fault Detector".
3.  **Sulphates (Positive):** Acts as a quality preservative.

### 3.5. ROC Curve & Cross-Validation Stability
![Red ROC](red_wine/outputs/red_rf_roc_curve.png)
*Figure 2: ROC Curve (AUC = 0.87) demonstrating excellent class separation ability.*

![Red CV Distribution](red_wine/outputs/red_cv_distribution.png)
*Figure 3: 10-Fold CV F1-Score Distribution showing model stability.*

### 3.6. Learning Curve & Calibration
![Red Learning Curve](red_wine/outputs/red_rf_learning_curve.png)
*Figure 4: Learning Curve showing Train vs CV F1-Score. No severe overfitting observed.*

![Red Calibration](red_wine/outputs/red_rf_calibration_curve.png)
*Figure 5: Calibration Curve showing probability reliability.*

---

# 4. Part II: White Wine Analysis ü•Ç

### 4.1. Executive Results (White)
*   **Champion Model:** Random Forest (Optimized)
*   **Accuracy:** **81.34%**
*   **F1-Score:** **0.60** (Strong performance on unique data)

**Full Metrics Comparison:**

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|:------|:--------:|:---------:|:------:|:--------:|:-------:|
| Logistic Regression | 74.3% | 0.43 | 0.78 | 0.56 | 0.82 |
| Random Forest (Tuned) | **81.3%** | 0.54 | 0.68 | **0.60** | 0.85 |
| XGBoost | 77.9% | 0.48 | 0.73 | 0.58 | 0.85 |

**Hyperparameter Tuning (GridSearchCV):**
- `n_estimators`: [100, 200] ‚Üí **Best: 200**
- `max_depth`: [10, 20] ‚Üí **Best: 10**
- `min_samples_split`: [5, 10] ‚Üí **Best: 5**
- Scoring: F1-Score with 5-Fold CV

### 4.2. Data Exploration (EDA)
![White Correlation](white_wine/outputs/03_correlation_heatmap_v3.png)
*Unlike Red wine, Density shows a strong negative correlation here.*

**Class Distribution:**
![White Distribution](white_wine/outputs/07_class_distribution_v3.png)
*White wine has a slightly higher proportion of "Good" wines (~20%) compared to Red (~14%).*

### 4.3. Model Performance
**Confusion Matrix:**
![White Confusion](white_wine/outputs/white_rf_confusion_matrix_v3.png)
*   **Recall is Higher (~69%):** The White Wine model is more "generous" ‚Äì it discovers a larger portion of good wines compared to the Red model.

### 4.4. Feature Importance Analysis (SHAP)
![White SHAP Summary](white_wine/outputs/white_shap_summary_dot.png)
*Figure 4: SHAP Summary Plot for White Wine Model.*

**Chemical Decode:**
1.  **Alcohol:** Still King.
2.  **Density:** **Critical differentiator.** Lower density (lighter body) is preferred in white wines.
3.  **Free Sulfur Dioxide:** Complex. It needs to be "just right" (Goldilocks zone).

### 4.5. ROC Curve & Cross-Validation Stability
![White ROC](white_wine/outputs/white_rf_roc_curve.png)
*Figure 5: ROC Curve (AUC = 0.85) for White Wine model.*

![White CV Distribution](white_wine/outputs/white_cv_distribution.png)
*Figure 6: 10-Fold CV F1-Score Distribution showing consistent performance.*

### 4.6. Learning Curve & Calibration
![White Learning Curve](white_wine/outputs/white_rf_learning_curve.png)
*Figure 7: Learning Curve confirming no severe overfitting.*

![White Calibration](white_wine/outputs/white_rf_calibration_curve.png)
*Figure 8: Calibration Curve showing probability reliability.*

---

# 5. Part III: Combined Dataset Analysis & Generalizability Check ‚öóÔ∏è
We conducted a rigorous experiment to answer a critical scientific question: **"Is wine quality subject to Simpson's Paradox?"** (i.e. Do the rules of quality flip between Red and White wines?).

### 5.1. Experimental Setup
*   **Merge:** Concatenated Red (1,599) and White (4,898) samples.
*   **Feature Engineering:** Added `type` column (Red=0, White=1).
*   **Hypothesis:** If chemical rules differ (e.g., Sugar is good for White but bad for Red), the model *must* use `type` as a top splitting feature to handle this interaction.

### 5.2. The "Type" Feature Verdict
![Combined Importance](combined_analysis/outputs/combined_feature_importance.png)

**Result:** The `type` feature is **Irrelevant** (Ranked last, Importance $\approx$ 0).
**Scientific Inference:**
1.  **No Simpson's Paradox:** The fact that the model ignores `type` proves that the primary drivers (Alcohol, Acidity) have the **same polarity** for both wines. High Volatile Acidity is universally bad; High Alcohol is universally good.
2.  **Universal Quality:** Quality is defined by *chemical balance*, independent of grape variety.

*Figure 7: Feature Importance including the 'type' variable (ranked last).*

### 5.3. Performance Analysis (Model Benchmark)
We trained both Random Forest (Standard Pipeline) and XGBoost (Optimized) on the combined data to ensure robust conclusions.

| Model | Accuracy | F1-Score | Interpretation |
| :--- | :--- | :--- | :--- |
| **Random Forest** | **84.30%** | **0.4014** | Stable but conservative. |
| **XGBoost** | **81.02%** | **0.5844** | **Statistically Significant Improvement (p < 0.05).** |

**Conclusion:** Merging datasets acts as valid **Data Augmentation**. The XGBoost model successfully handles the domain shift, achieving an F1-Score (0.58) comparable to the standalone White Wine model (0.60).
**Statistical Verification:** A Paired T-Test confirmed that XGBoost significantly outperforms Random Forest on the combined dataset ($p < 0.05$), likely due to its superior handling of the complex interaction between Red/White domains.

### 5.4. Combined Confusion Matrix
![Combined CM](combined_analysis/outputs/combined_confusion_matrix.png)
*The unified model holds up, proving a single AI agent could monitor a whole winery.*

### 5.5. Combined ROC Curve & Model Stability
![Combined ROC](combined_analysis/outputs/combined_xgb_roc_curve.png)
*Figure 8: ROC Curve for Combined XGBoost model (AUC = 0.85).*

![Combined CV](combined_analysis/outputs/combined_cv_distribution.png)
*Figure 9: CV F1-Score Distribution confirming XGBoost's statistical superiority.*

---

# 6. Part IV: Exact Quality Score Prediction (Regression) üìâ
While binary classification (Good/Bad) is useful for filtering, a granular scoring system (0-10 scale) allows for finer inventory grading. We implemented a **Regression Pipeline** to predict the exact quality score.

### 6.1. Methodology
*   **Target:** `Quality` (Continuous variable 0-10).
*   **Algorithm:** **Random Forest Regressor** (chosen for its ability to capture non-linear relationships in chemical data without extensive scaling).
*   **Scaling:** Standard Scaling applied (though not strictly necessary for trees, it stabilizes the variance).

### 6.2. Regression Performance Results
The model was evaluated using RMSE (Root Mean Squared Error) to quantify the average deviation in points.

| Metric | Score | Interpretation |
|:-------|:-----:|:---------------|
| **MSE** | **0.3687** | Mean Squared Error |
| **RMSE** | **0.6072** | **Key Metric:** Predictions are "off" by only ~0.6 points on average. |
| **MAE** | **0.4287** | Mean Absolute Error |
| **$R^2$ Score** | **0.48** | Explains ~48% of the variance in quality scores. |

**Insight:** A RMSE of ~0.6 means if a professional sommelier rates a wine **7.0**, our AI predicts between **6.4** and **7.6**. This is "Human-Expert Level" consistency, acceptable for automated grading aids.

### 6.3. Prediction Analysis
**Actual vs. Predicted:**
![Actual vs Predicted](score_prediction/outputs/actual_vs_predicted.png)
*Figure 10: The model shows a strong linear trend. It avoids extreme hallucinations, staying close to the diagonal ideal line.*

**Residuals Distribution:**
![Residuals](score_prediction/outputs/residuals_distribution.png)
*Figure 11: The error distribution is Normal (Bell-shaped) and centered at 0, proving the model is unbiased.*

---

# 7. Part V: Blind Type Identification (Classification) üç∑ü•Ç
Can an AI classify a wine as Red or White **solely based on its chemical properties**, without seeing the color? This "Blind Test" validates the chemical distinctiveness of the two types.

### 7.1. Experimental Protocol
*   **Objective:** Predict `type` (Red=0, White=1).
*   **Constraint:** The `quality` column was **DROPPED** to ensure prediction relies solely on physicochemical properties.
*   **Algorithm:** **XGBoost Classifier** (Gradient Boosting).

### 7.2. Classification Results (Perfect Separation)
The model achieved near-perfect performance, proving Red and White wines are chemically distinct universes.

*   **Accuracy:** **99.62%**
*   **ROC-AUC:** **0.999**

**Confusion Matrix:**
![Type CM](type_prediction/outputs/confusion_matrix.png)
*Figure 12: Only ~5 misclassifications out of thousands. The chemical signature of Red vs White is undeniable.*

### 7.3. What defines a "Red" vs "White" chemically?
Using Feature Importance, we identified the molecular differentiators:

![Type Features](type_prediction/outputs/feature_importance.png)

1.  **Total Sulfur Dioxide:** The #1 differentiator. White wines typically require higher SO‚ÇÇ for preservation (antioxidant) as they lack the natural tannins found in Red wines.
2.  **Volatile Acidity:** Reds naturally have higher volatile acidity boundaries.
3.  **Chlorides/Density:** Significant structural differences.

---

# 8. Comparative Conclusion & Recommendations

### 8.1. Best Model for Production?
*   **For Red Wine:** **Random Forest (Tuned)**. It offers the highest safety (lowest False Positives). Ideally used for filtering out bad batches.
*   **For White Wine:** **Random Forest**. It achieved the highest F1-Score (0.60) and Accuracy (81.3%), proving robust against the larger variations in the white wine dataset.

### 8.2. Final Assessment and Future Outlook
This project successfully demonstrates:
1.  **Academic Rigor:** No leakage, proper cross-validation.
2.  **Technical Depth:** Implementation of advanced Boosting algorithms and Explainable AI.
3.  **Business Intelligence:** Threshold tuning for commercial viability.

The system is robust, scientifically validated, and ready for deployment.

### 8.3. Future Outlook: Cost-Sensitive Learning
While this study optimized for the F1-Score, future commercial iterations should implement a **Cost Matrix**. In a real-world winery, the cost of a "False Positive" (Labeling bad wine as Premium -> Brand Damage) is likely higher than a "False Negative" (Labeling Premium as Standard -> Missed Opportunity). Integrating this asymmetric cost directly into the loss function would align the model perfectly with business profit maximization.

---
*End of Documentation.*
